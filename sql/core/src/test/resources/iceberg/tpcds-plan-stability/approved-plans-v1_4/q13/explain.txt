== Physical Plan ==
* HashAggregate (33)
+- Exchange (32)
   +- * HashAggregate (31)
      +- * Project (30)
         +- * BroadcastHashJoin Inner BuildRight (29)
            :- * Project (24)
            :  +- * BroadcastHashJoin Inner BuildRight (23)
            :     :- * Project (18)
            :     :  +- * BroadcastHashJoin Inner BuildRight (17)
            :     :     :- * Project (15)
            :     :     :  +- * BroadcastHashJoin Inner BuildRight (14)
            :     :     :     :- * Project (9)
            :     :     :     :  +- * BroadcastHashJoin Inner BuildRight (8)
            :     :     :     :     :- * Project (3)
            :     :     :     :     :  +- * Filter (2)
            :     :     :     :     :     +- BatchScan spark_catalog.default.store_sales (1)
            :     :     :     :     +- BroadcastExchange (7)
            :     :     :     :        +- * Project (6)
            :     :     :     :           +- * Filter (5)
            :     :     :     :              +- BatchScan spark_catalog.default.store (4)
            :     :     :     +- BroadcastExchange (13)
            :     :     :        +- * Project (12)
            :     :     :           +- * Filter (11)
            :     :     :              +- BatchScan spark_catalog.default.customer_address (10)
            :     :     +- ReusedExchange (16)
            :     +- BroadcastExchange (22)
            :        +- * Project (21)
            :           +- * Filter (20)
            :              +- BatchScan spark_catalog.default.customer_demographics (19)
            +- BroadcastExchange (28)
               +- * Project (27)
                  +- * Filter (26)
                     +- BatchScan spark_catalog.default.household_demographics (25)


(1) BatchScan spark_catalog.default.store_sales
Output [10]: [ss_sold_date_sk#1, ss_cdemo_sk#2, ss_hdemo_sk#3, ss_addr_sk#4, ss_store_sk#5, ss_quantity#6, ss_sales_price#7, ss_ext_sales_price#8, ss_ext_wholesale_cost#9, ss_net_profit#10]
spark_catalog.default.store_sales [scan class = SparkBatchQueryScan] [filters=ss_store_sk IS NOT NULL, ss_addr_sk IS NOT NULL, ss_sold_date_sk IS NOT NULL, ss_cdemo_sk IS NOT NULL, ss_hdemo_sk IS NOT NULL, (((ss_net_profit >= 100.00 AND ss_net_profit <= 200.00) OR (ss_net_profit >= 150.00 AND ss_net_profit <= 300.00)) OR (ss_net_profit >= 50.00 AND ss_net_profit <= 250.00)), (((ss_sales_price >= 100.00 AND ss_sales_price <= 150.00) OR (ss_sales_price >= 50.00 AND ss_sales_price <= 100.00)) OR (ss_sales_price >= 150.00 AND ss_sales_price <= 200.00))], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(2) Filter [codegen id : 6]
Input [10]: [ss_sold_date_sk#1, ss_cdemo_sk#2, ss_hdemo_sk#3, ss_addr_sk#4, ss_store_sk#5, ss_quantity#6, ss_sales_price#7, ss_ext_sales_price#8, ss_ext_wholesale_cost#9, ss_net_profit#10]
Condition : ((((((isnotnull(ss_store_sk#5) AND isnotnull(ss_addr_sk#4)) AND isnotnull(ss_sold_date_sk#1)) AND isnotnull(ss_cdemo_sk#2)) AND isnotnull(ss_hdemo_sk#3)) AND ((((ss_net_profit#10 >= 100.00) AND (ss_net_profit#10 <= 200.00)) OR ((ss_net_profit#10 >= 150.00) AND (ss_net_profit#10 <= 300.00))) OR ((ss_net_profit#10 >= 50.00) AND (ss_net_profit#10 <= 250.00)))) AND ((((ss_sales_price#7 >= 100.00) AND (ss_sales_price#7 <= 150.00)) OR ((ss_sales_price#7 >= 50.00) AND (ss_sales_price#7 <= 100.00))) OR ((ss_sales_price#7 >= 150.00) AND (ss_sales_price#7 <= 200.00))))

(3) Project [codegen id : 6]
Output [10]: [ss_sold_date_sk#1, ss_cdemo_sk#2, ss_hdemo_sk#3, ss_addr_sk#4, ss_store_sk#5, ss_quantity#6, ss_sales_price#7, ss_ext_sales_price#8, ss_ext_wholesale_cost#9, ss_net_profit#10]
Input [10]: [ss_sold_date_sk#1, ss_cdemo_sk#2, ss_hdemo_sk#3, ss_addr_sk#4, ss_store_sk#5, ss_quantity#6, ss_sales_price#7, ss_ext_sales_price#8, ss_ext_wholesale_cost#9, ss_net_profit#10]

(4) BatchScan spark_catalog.default.store
Output [1]: [s_store_sk#11]
spark_catalog.default.store [scan class = SparkBatchQueryScan] [filters=s_store_sk IS NOT NULL], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(5) Filter [codegen id : 1]
Input [1]: [s_store_sk#11]
Condition : isnotnull(s_store_sk#11)

(6) Project [codegen id : 1]
Output [1]: [s_store_sk#11]
Input [1]: [s_store_sk#11]

(7) BroadcastExchange
Input [1]: [s_store_sk#11]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=1]

(8) BroadcastHashJoin [codegen id : 6]
Left keys [1]: [ss_store_sk#5]
Right keys [1]: [s_store_sk#11]
Join condition: None

(9) Project [codegen id : 6]
Output [9]: [ss_sold_date_sk#1, ss_cdemo_sk#2, ss_hdemo_sk#3, ss_addr_sk#4, ss_quantity#6, ss_sales_price#7, ss_ext_sales_price#8, ss_ext_wholesale_cost#9, ss_net_profit#10]
Input [11]: [ss_sold_date_sk#1, ss_cdemo_sk#2, ss_hdemo_sk#3, ss_addr_sk#4, ss_store_sk#5, ss_quantity#6, ss_sales_price#7, ss_ext_sales_price#8, ss_ext_wholesale_cost#9, ss_net_profit#10, s_store_sk#11]

(10) BatchScan spark_catalog.default.customer_address
Output [3]: [ca_address_sk#12, ca_state#13, ca_country#14]
spark_catalog.default.customer_address [scan class = SparkBatchQueryScan] [filters=ca_country IS NOT NULL, ca_country = 'United States', ca_address_sk IS NOT NULL, ((ca_state IN ('TX', 'OH') OR ca_state IN ('OR', 'NM', 'KY')) OR ca_state IN ('VA', 'TX', 'MS'))], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(11) Filter [codegen id : 2]
Input [3]: [ca_address_sk#12, ca_state#13, ca_country#14]
Condition : (((isnotnull(ca_country#14) AND (ca_country#14 = United States)) AND isnotnull(ca_address_sk#12)) AND ((ca_state#13 IN (TX,OH) OR ca_state#13 IN (OR,NM,KY)) OR ca_state#13 IN (VA,TX,MS)))

(12) Project [codegen id : 2]
Output [2]: [ca_address_sk#12, ca_state#13]
Input [3]: [ca_address_sk#12, ca_state#13, ca_country#14]

(13) BroadcastExchange
Input [2]: [ca_address_sk#12, ca_state#13]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=2]

(14) BroadcastHashJoin [codegen id : 6]
Left keys [1]: [ss_addr_sk#4]
Right keys [1]: [ca_address_sk#12]
Join condition: ((((ca_state#13 IN (TX,OH) AND (ss_net_profit#10 >= 100.00)) AND (ss_net_profit#10 <= 200.00)) OR ((ca_state#13 IN (OR,NM,KY) AND (ss_net_profit#10 >= 150.00)) AND (ss_net_profit#10 <= 300.00))) OR ((ca_state#13 IN (VA,TX,MS) AND (ss_net_profit#10 >= 50.00)) AND (ss_net_profit#10 <= 250.00)))

(15) Project [codegen id : 6]
Output [7]: [ss_sold_date_sk#1, ss_cdemo_sk#2, ss_hdemo_sk#3, ss_quantity#6, ss_sales_price#7, ss_ext_sales_price#8, ss_ext_wholesale_cost#9]
Input [11]: [ss_sold_date_sk#1, ss_cdemo_sk#2, ss_hdemo_sk#3, ss_addr_sk#4, ss_quantity#6, ss_sales_price#7, ss_ext_sales_price#8, ss_ext_wholesale_cost#9, ss_net_profit#10, ca_address_sk#12, ca_state#13]

(16) ReusedExchange [Reuses operator id: 37]
Output [1]: [d_date_sk#15]

(17) BroadcastHashJoin [codegen id : 6]
Left keys [1]: [ss_sold_date_sk#1]
Right keys [1]: [d_date_sk#15]
Join condition: None

(18) Project [codegen id : 6]
Output [6]: [ss_cdemo_sk#2, ss_hdemo_sk#3, ss_quantity#6, ss_sales_price#7, ss_ext_sales_price#8, ss_ext_wholesale_cost#9]
Input [8]: [ss_sold_date_sk#1, ss_cdemo_sk#2, ss_hdemo_sk#3, ss_quantity#6, ss_sales_price#7, ss_ext_sales_price#8, ss_ext_wholesale_cost#9, d_date_sk#15]

(19) BatchScan spark_catalog.default.customer_demographics
Output [3]: [cd_demo_sk#16, cd_marital_status#17, cd_education_status#18]
spark_catalog.default.customer_demographics [scan class = SparkBatchQueryScan] [filters=cd_demo_sk IS NOT NULL, (((cd_marital_status = 'M' AND cd_education_status = 'Advanced Degree') OR (cd_marital_status = 'S' AND cd_education_status = 'College')) OR (cd_marital_status = 'W' AND cd_education_status = '2 yr Degree'))], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(20) Filter [codegen id : 4]
Input [3]: [cd_demo_sk#16, cd_marital_status#17, cd_education_status#18]
Condition : (isnotnull(cd_demo_sk#16) AND ((((cd_marital_status#17 = M) AND (cd_education_status#18 = Advanced Degree)) OR ((cd_marital_status#17 = S) AND (cd_education_status#18 = College))) OR ((cd_marital_status#17 = W) AND (cd_education_status#18 = 2 yr Degree))))

(21) Project [codegen id : 4]
Output [3]: [cd_demo_sk#16, cd_marital_status#17, cd_education_status#18]
Input [3]: [cd_demo_sk#16, cd_marital_status#17, cd_education_status#18]

(22) BroadcastExchange
Input [3]: [cd_demo_sk#16, cd_marital_status#17, cd_education_status#18]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=3]

(23) BroadcastHashJoin [codegen id : 6]
Left keys [1]: [ss_cdemo_sk#2]
Right keys [1]: [cd_demo_sk#16]
Join condition: ((((((cd_marital_status#17 = M) AND (cd_education_status#18 = Advanced Degree)) AND (ss_sales_price#7 >= 100.00)) AND (ss_sales_price#7 <= 150.00)) OR ((((cd_marital_status#17 = S) AND (cd_education_status#18 = College)) AND (ss_sales_price#7 >= 50.00)) AND (ss_sales_price#7 <= 100.00))) OR ((((cd_marital_status#17 = W) AND (cd_education_status#18 = 2 yr Degree)) AND (ss_sales_price#7 >= 150.00)) AND (ss_sales_price#7 <= 200.00)))

(24) Project [codegen id : 6]
Output [7]: [ss_hdemo_sk#3, ss_quantity#6, ss_sales_price#7, ss_ext_sales_price#8, ss_ext_wholesale_cost#9, cd_marital_status#17, cd_education_status#18]
Input [9]: [ss_cdemo_sk#2, ss_hdemo_sk#3, ss_quantity#6, ss_sales_price#7, ss_ext_sales_price#8, ss_ext_wholesale_cost#9, cd_demo_sk#16, cd_marital_status#17, cd_education_status#18]

(25) BatchScan spark_catalog.default.household_demographics
Output [2]: [hd_demo_sk#19, hd_dep_count#20]
spark_catalog.default.household_demographics [scan class = SparkBatchQueryScan] [filters=hd_demo_sk IS NOT NULL, ((hd_dep_count = 3 OR hd_dep_count = 1) OR hd_dep_count = 1)], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(26) Filter [codegen id : 5]
Input [2]: [hd_demo_sk#19, hd_dep_count#20]
Condition : (isnotnull(hd_demo_sk#19) AND ((hd_dep_count#20 = 3) OR (hd_dep_count#20 = 1)))

(27) Project [codegen id : 5]
Output [2]: [hd_demo_sk#19, hd_dep_count#20]
Input [2]: [hd_demo_sk#19, hd_dep_count#20]

(28) BroadcastExchange
Input [2]: [hd_demo_sk#19, hd_dep_count#20]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=4]

(29) BroadcastHashJoin [codegen id : 6]
Left keys [1]: [ss_hdemo_sk#3]
Right keys [1]: [hd_demo_sk#19]
Join condition: (((((((cd_marital_status#17 = M) AND (cd_education_status#18 = Advanced Degree)) AND (ss_sales_price#7 >= 100.00)) AND (ss_sales_price#7 <= 150.00)) AND (hd_dep_count#20 = 3)) OR (((((cd_marital_status#17 = S) AND (cd_education_status#18 = College)) AND (ss_sales_price#7 >= 50.00)) AND (ss_sales_price#7 <= 100.00)) AND (hd_dep_count#20 = 1))) OR (((((cd_marital_status#17 = W) AND (cd_education_status#18 = 2 yr Degree)) AND (ss_sales_price#7 >= 150.00)) AND (ss_sales_price#7 <= 200.00)) AND (hd_dep_count#20 = 1)))

(30) Project [codegen id : 6]
Output [3]: [ss_quantity#6, ss_ext_sales_price#8, ss_ext_wholesale_cost#9]
Input [9]: [ss_hdemo_sk#3, ss_quantity#6, ss_sales_price#7, ss_ext_sales_price#8, ss_ext_wholesale_cost#9, cd_marital_status#17, cd_education_status#18, hd_demo_sk#19, hd_dep_count#20]

(31) HashAggregate [codegen id : 6]
Input [3]: [ss_quantity#6, ss_ext_sales_price#8, ss_ext_wholesale_cost#9]
Keys: []
Functions [4]: [partial_avg(ss_quantity#6), partial_avg(UnscaledValue(ss_ext_sales_price#8)), partial_avg(UnscaledValue(ss_ext_wholesale_cost#9)), partial_sum(UnscaledValue(ss_ext_wholesale_cost#9))]
Aggregate Attributes [7]: [sum#21, count#22, sum#23, count#24, sum#25, count#26, sum#27]
Results [7]: [sum#28, count#29, sum#30, count#31, sum#32, count#33, sum#34]

(32) Exchange
Input [7]: [sum#28, count#29, sum#30, count#31, sum#32, count#33, sum#34]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=5]

(33) HashAggregate [codegen id : 7]
Input [7]: [sum#28, count#29, sum#30, count#31, sum#32, count#33, sum#34]
Keys: []
Functions [4]: [avg(ss_quantity#6), avg(UnscaledValue(ss_ext_sales_price#8)), avg(UnscaledValue(ss_ext_wholesale_cost#9)), sum(UnscaledValue(ss_ext_wholesale_cost#9))]
Aggregate Attributes [4]: [avg(ss_quantity#6)#35, avg(UnscaledValue(ss_ext_sales_price#8))#36, avg(UnscaledValue(ss_ext_wholesale_cost#9))#37, sum(UnscaledValue(ss_ext_wholesale_cost#9))#38]
Results [4]: [avg(ss_quantity#6)#35 AS avg(ss_quantity)#39, cast((avg(UnscaledValue(ss_ext_sales_price#8))#36 / 100.0) as decimal(11,6)) AS avg(ss_ext_sales_price)#40, cast((avg(UnscaledValue(ss_ext_wholesale_cost#9))#37 / 100.0) as decimal(11,6)) AS avg(ss_ext_wholesale_cost)#41, MakeDecimal(sum(UnscaledValue(ss_ext_wholesale_cost#9))#38,17,2) AS sum(ss_ext_wholesale_cost)#42]

===== Subqueries =====

Subquery:1 Hosting operator id = 1 Hosting Expression = ss_sold_date_sk#1 IN dynamicpruning#43
BroadcastExchange (37)
+- * Project (36)
   +- * Filter (35)
      +- BatchScan spark_catalog.default.date_dim (34)


(34) BatchScan spark_catalog.default.date_dim
Output [2]: [d_date_sk#15, d_year#44]
spark_catalog.default.date_dim [scan class = SparkBatchQueryScan] [filters=d_year IS NOT NULL, d_year = 2001, d_date_sk IS NOT NULL], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(35) Filter [codegen id : 1]
Input [2]: [d_date_sk#15, d_year#44]
Condition : ((isnotnull(d_year#44) AND (d_year#44 = 2001)) AND isnotnull(d_date_sk#15))

(36) Project [codegen id : 1]
Output [1]: [d_date_sk#15]
Input [2]: [d_date_sk#15, d_year#44]

(37) BroadcastExchange
Input [1]: [d_date_sk#15]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=6]


