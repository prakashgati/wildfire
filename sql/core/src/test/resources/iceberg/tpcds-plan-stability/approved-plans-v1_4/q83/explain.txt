== Physical Plan ==
TakeOrderedAndProject (46)
+- * Project (45)
   +- * BroadcastHashJoin Inner BuildRight (44)
      :- * Project (30)
      :  +- * BroadcastHashJoin Inner BuildRight (29)
      :     :- * HashAggregate (15)
      :     :  +- Exchange (14)
      :     :     +- * HashAggregate (13)
      :     :        +- * Project (12)
      :     :           +- * BroadcastHashJoin Inner BuildRight (11)
      :     :              :- * Project (9)
      :     :              :  +- * BroadcastHashJoin Inner BuildRight (8)
      :     :              :     :- * Project (3)
      :     :              :     :  +- * Filter (2)
      :     :              :     :     +- BatchScan spark_catalog.default.store_returns (1)
      :     :              :     +- BroadcastExchange (7)
      :     :              :        +- * Project (6)
      :     :              :           +- * Filter (5)
      :     :              :              +- BatchScan spark_catalog.default.item (4)
      :     :              +- ReusedExchange (10)
      :     +- BroadcastExchange (28)
      :        +- * HashAggregate (27)
      :           +- Exchange (26)
      :              +- * HashAggregate (25)
      :                 +- * Project (24)
      :                    +- * BroadcastHashJoin Inner BuildRight (23)
      :                       :- * Project (21)
      :                       :  +- * BroadcastHashJoin Inner BuildRight (20)
      :                       :     :- * Project (18)
      :                       :     :  +- * Filter (17)
      :                       :     :     +- BatchScan spark_catalog.default.catalog_returns (16)
      :                       :     +- ReusedExchange (19)
      :                       +- ReusedExchange (22)
      +- BroadcastExchange (43)
         +- * HashAggregate (42)
            +- Exchange (41)
               +- * HashAggregate (40)
                  +- * Project (39)
                     +- * BroadcastHashJoin Inner BuildRight (38)
                        :- * Project (36)
                        :  +- * BroadcastHashJoin Inner BuildRight (35)
                        :     :- * Project (33)
                        :     :  +- * Filter (32)
                        :     :     +- BatchScan spark_catalog.default.web_returns (31)
                        :     +- ReusedExchange (34)
                        +- ReusedExchange (37)


(1) BatchScan spark_catalog.default.store_returns
Output [3]: [sr_returned_date_sk#1, sr_item_sk#2, sr_return_quantity#3]
spark_catalog.default.store_returns [scan class = SparkBatchQueryScan] [filters=sr_item_sk IS NOT NULL, sr_returned_date_sk IS NOT NULL], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(2) Filter [codegen id : 5]
Input [3]: [sr_returned_date_sk#1, sr_item_sk#2, sr_return_quantity#3]
Condition : (isnotnull(sr_item_sk#2) AND isnotnull(sr_returned_date_sk#1))

(3) Project [codegen id : 5]
Output [3]: [sr_returned_date_sk#1, sr_item_sk#2, sr_return_quantity#3]
Input [3]: [sr_returned_date_sk#1, sr_item_sk#2, sr_return_quantity#3]

(4) BatchScan spark_catalog.default.item
Output [2]: [i_item_sk#4, i_item_id#5]
spark_catalog.default.item [scan class = SparkBatchQueryScan] [filters=i_item_sk IS NOT NULL, i_item_id IS NOT NULL], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(5) Filter [codegen id : 1]
Input [2]: [i_item_sk#4, i_item_id#5]
Condition : (isnotnull(i_item_sk#4) AND isnotnull(i_item_id#5))

(6) Project [codegen id : 1]
Output [2]: [i_item_sk#4, i_item_id#5]
Input [2]: [i_item_sk#4, i_item_id#5]

(7) BroadcastExchange
Input [2]: [i_item_sk#4, i_item_id#5]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=1]

(8) BroadcastHashJoin [codegen id : 5]
Left keys [1]: [sr_item_sk#2]
Right keys [1]: [i_item_sk#4]
Join condition: None

(9) Project [codegen id : 5]
Output [3]: [sr_returned_date_sk#1, sr_return_quantity#3, i_item_id#5]
Input [5]: [sr_returned_date_sk#1, sr_item_sk#2, sr_return_quantity#3, i_item_sk#4, i_item_id#5]

(10) ReusedExchange [Reuses operator id: 61]
Output [1]: [d_date_sk#6]

(11) BroadcastHashJoin [codegen id : 5]
Left keys [1]: [sr_returned_date_sk#1]
Right keys [1]: [d_date_sk#6]
Join condition: None

(12) Project [codegen id : 5]
Output [2]: [sr_return_quantity#3, i_item_id#5]
Input [4]: [sr_returned_date_sk#1, sr_return_quantity#3, i_item_id#5, d_date_sk#6]

(13) HashAggregate [codegen id : 5]
Input [2]: [sr_return_quantity#3, i_item_id#5]
Keys [1]: [i_item_id#5]
Functions [1]: [partial_sum(sr_return_quantity#3)]
Aggregate Attributes [1]: [sum#7]
Results [2]: [i_item_id#5, sum#8]

(14) Exchange
Input [2]: [i_item_id#5, sum#8]
Arguments: hashpartitioning(i_item_id#5, 5), ENSURE_REQUIREMENTS, [plan_id=2]

(15) HashAggregate [codegen id : 18]
Input [2]: [i_item_id#5, sum#8]
Keys [1]: [i_item_id#5]
Functions [1]: [sum(sr_return_quantity#3)]
Aggregate Attributes [1]: [sum(sr_return_quantity#3)#9]
Results [2]: [i_item_id#5 AS item_id#10, sum(sr_return_quantity#3)#9 AS sr_item_qty#11]

(16) BatchScan spark_catalog.default.catalog_returns
Output [3]: [cr_returned_date_sk#12, cr_item_sk#13, cr_return_quantity#14]
spark_catalog.default.catalog_returns [scan class = SparkBatchQueryScan] [filters=cr_item_sk IS NOT NULL, cr_returned_date_sk IS NOT NULL], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(17) Filter [codegen id : 10]
Input [3]: [cr_returned_date_sk#12, cr_item_sk#13, cr_return_quantity#14]
Condition : (isnotnull(cr_item_sk#13) AND isnotnull(cr_returned_date_sk#12))

(18) Project [codegen id : 10]
Output [3]: [cr_returned_date_sk#12, cr_item_sk#13, cr_return_quantity#14]
Input [3]: [cr_returned_date_sk#12, cr_item_sk#13, cr_return_quantity#14]

(19) ReusedExchange [Reuses operator id: 7]
Output [2]: [i_item_sk#15, i_item_id#16]

(20) BroadcastHashJoin [codegen id : 10]
Left keys [1]: [cr_item_sk#13]
Right keys [1]: [i_item_sk#15]
Join condition: None

(21) Project [codegen id : 10]
Output [3]: [cr_returned_date_sk#12, cr_return_quantity#14, i_item_id#16]
Input [5]: [cr_returned_date_sk#12, cr_item_sk#13, cr_return_quantity#14, i_item_sk#15, i_item_id#16]

(22) ReusedExchange [Reuses operator id: 61]
Output [1]: [d_date_sk#17]

(23) BroadcastHashJoin [codegen id : 10]
Left keys [1]: [cr_returned_date_sk#12]
Right keys [1]: [d_date_sk#17]
Join condition: None

(24) Project [codegen id : 10]
Output [2]: [cr_return_quantity#14, i_item_id#16]
Input [4]: [cr_returned_date_sk#12, cr_return_quantity#14, i_item_id#16, d_date_sk#17]

(25) HashAggregate [codegen id : 10]
Input [2]: [cr_return_quantity#14, i_item_id#16]
Keys [1]: [i_item_id#16]
Functions [1]: [partial_sum(cr_return_quantity#14)]
Aggregate Attributes [1]: [sum#18]
Results [2]: [i_item_id#16, sum#19]

(26) Exchange
Input [2]: [i_item_id#16, sum#19]
Arguments: hashpartitioning(i_item_id#16, 5), ENSURE_REQUIREMENTS, [plan_id=3]

(27) HashAggregate [codegen id : 11]
Input [2]: [i_item_id#16, sum#19]
Keys [1]: [i_item_id#16]
Functions [1]: [sum(cr_return_quantity#14)]
Aggregate Attributes [1]: [sum(cr_return_quantity#14)#20]
Results [2]: [i_item_id#16 AS item_id#21, sum(cr_return_quantity#14)#20 AS cr_item_qty#22]

(28) BroadcastExchange
Input [2]: [item_id#21, cr_item_qty#22]
Arguments: HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=4]

(29) BroadcastHashJoin [codegen id : 18]
Left keys [1]: [item_id#10]
Right keys [1]: [item_id#21]
Join condition: None

(30) Project [codegen id : 18]
Output [3]: [item_id#10, sr_item_qty#11, cr_item_qty#22]
Input [4]: [item_id#10, sr_item_qty#11, item_id#21, cr_item_qty#22]

(31) BatchScan spark_catalog.default.web_returns
Output [3]: [wr_returned_date_sk#23, wr_item_sk#24, wr_return_quantity#25]
spark_catalog.default.web_returns [scan class = SparkBatchQueryScan] [filters=wr_item_sk IS NOT NULL, wr_returned_date_sk IS NOT NULL], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(32) Filter [codegen id : 16]
Input [3]: [wr_returned_date_sk#23, wr_item_sk#24, wr_return_quantity#25]
Condition : (isnotnull(wr_item_sk#24) AND isnotnull(wr_returned_date_sk#23))

(33) Project [codegen id : 16]
Output [3]: [wr_returned_date_sk#23, wr_item_sk#24, wr_return_quantity#25]
Input [3]: [wr_returned_date_sk#23, wr_item_sk#24, wr_return_quantity#25]

(34) ReusedExchange [Reuses operator id: 7]
Output [2]: [i_item_sk#26, i_item_id#27]

(35) BroadcastHashJoin [codegen id : 16]
Left keys [1]: [wr_item_sk#24]
Right keys [1]: [i_item_sk#26]
Join condition: None

(36) Project [codegen id : 16]
Output [3]: [wr_returned_date_sk#23, wr_return_quantity#25, i_item_id#27]
Input [5]: [wr_returned_date_sk#23, wr_item_sk#24, wr_return_quantity#25, i_item_sk#26, i_item_id#27]

(37) ReusedExchange [Reuses operator id: 61]
Output [1]: [d_date_sk#28]

(38) BroadcastHashJoin [codegen id : 16]
Left keys [1]: [wr_returned_date_sk#23]
Right keys [1]: [d_date_sk#28]
Join condition: None

(39) Project [codegen id : 16]
Output [2]: [wr_return_quantity#25, i_item_id#27]
Input [4]: [wr_returned_date_sk#23, wr_return_quantity#25, i_item_id#27, d_date_sk#28]

(40) HashAggregate [codegen id : 16]
Input [2]: [wr_return_quantity#25, i_item_id#27]
Keys [1]: [i_item_id#27]
Functions [1]: [partial_sum(wr_return_quantity#25)]
Aggregate Attributes [1]: [sum#29]
Results [2]: [i_item_id#27, sum#30]

(41) Exchange
Input [2]: [i_item_id#27, sum#30]
Arguments: hashpartitioning(i_item_id#27, 5), ENSURE_REQUIREMENTS, [plan_id=5]

(42) HashAggregate [codegen id : 17]
Input [2]: [i_item_id#27, sum#30]
Keys [1]: [i_item_id#27]
Functions [1]: [sum(wr_return_quantity#25)]
Aggregate Attributes [1]: [sum(wr_return_quantity#25)#31]
Results [2]: [i_item_id#27 AS item_id#32, sum(wr_return_quantity#25)#31 AS wr_item_qty#33]

(43) BroadcastExchange
Input [2]: [item_id#32, wr_item_qty#33]
Arguments: HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=6]

(44) BroadcastHashJoin [codegen id : 18]
Left keys [1]: [item_id#10]
Right keys [1]: [item_id#32]
Join condition: None

(45) Project [codegen id : 18]
Output [8]: [item_id#10, sr_item_qty#11, (((cast(sr_item_qty#11 as double) / cast(((sr_item_qty#11 + cr_item_qty#22) + wr_item_qty#33) as double)) / 3.0) * 100.0) AS sr_dev#34, cr_item_qty#22, (((cast(cr_item_qty#22 as double) / cast(((sr_item_qty#11 + cr_item_qty#22) + wr_item_qty#33) as double)) / 3.0) * 100.0) AS cr_dev#35, wr_item_qty#33, (((cast(wr_item_qty#33 as double) / cast(((sr_item_qty#11 + cr_item_qty#22) + wr_item_qty#33) as double)) / 3.0) * 100.0) AS wr_dev#36, CheckOverflow((promote_precision(cast(((sr_item_qty#11 + cr_item_qty#22) + wr_item_qty#33) as decimal(21,1))) / 3.0), DecimalType(27,6)) AS average#37]
Input [5]: [item_id#10, sr_item_qty#11, cr_item_qty#22, item_id#32, wr_item_qty#33]

(46) TakeOrderedAndProject
Input [8]: [item_id#10, sr_item_qty#11, sr_dev#34, cr_item_qty#22, cr_dev#35, wr_item_qty#33, wr_dev#36, average#37]
Arguments: 100, [item_id#10 ASC NULLS FIRST, sr_item_qty#11 ASC NULLS FIRST], [item_id#10, sr_item_qty#11, sr_dev#34, cr_item_qty#22, cr_dev#35, wr_item_qty#33, wr_dev#36, average#37]

===== Subqueries =====

Subquery:1 Hosting operator id = 1 Hosting Expression = sr_returned_date_sk#1 IN dynamicpruning#38
BroadcastExchange (61)
+- * Project (60)
   +- * BroadcastHashJoin LeftSemi BuildRight (59)
      :- * Project (49)
      :  +- * Filter (48)
      :     +- BatchScan spark_catalog.default.date_dim (47)
      +- BroadcastExchange (58)
         +- * Project (57)
            +- * BroadcastHashJoin LeftSemi BuildRight (56)
               :- * Project (51)
               :  +- BatchScan spark_catalog.default.date_dim (50)
               +- BroadcastExchange (55)
                  +- * Project (54)
                     +- * Filter (53)
                        +- BatchScan spark_catalog.default.date_dim (52)


(47) BatchScan spark_catalog.default.date_dim
Output [2]: [d_date_sk#6, d_date#39]
spark_catalog.default.date_dim [scan class = SparkBatchQueryScan] [filters=d_date_sk IS NOT NULL], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(48) Filter [codegen id : 3]
Input [2]: [d_date_sk#6, d_date#39]
Condition : isnotnull(d_date_sk#6)

(49) Project [codegen id : 3]
Output [2]: [d_date_sk#6, d_date#39]
Input [2]: [d_date_sk#6, d_date#39]

(50) BatchScan spark_catalog.default.date_dim
Output [2]: [d_date#40, d_week_seq#41]
spark_catalog.default.date_dim [scan class = SparkBatchQueryScan] [filters=], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(51) Project [codegen id : 2]
Output [2]: [d_date#40, d_week_seq#41]
Input [2]: [d_date#40, d_week_seq#41]

(52) BatchScan spark_catalog.default.date_dim
Output [2]: [d_date#42, d_week_seq#43]
spark_catalog.default.date_dim [scan class = SparkBatchQueryScan] [filters=], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(53) Filter [codegen id : 1]
Input [2]: [d_date#42, d_week_seq#43]
Condition : cast(d_date#42 as string) IN (2000-06-30,2000-09-27,2000-11-17)

(54) Project [codegen id : 1]
Output [1]: [d_week_seq#43]
Input [2]: [d_date#42, d_week_seq#43]

(55) BroadcastExchange
Input [1]: [d_week_seq#43]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=7]

(56) BroadcastHashJoin [codegen id : 2]
Left keys [1]: [d_week_seq#41]
Right keys [1]: [d_week_seq#43]
Join condition: None

(57) Project [codegen id : 2]
Output [1]: [d_date#40]
Input [2]: [d_date#40, d_week_seq#41]

(58) BroadcastExchange
Input [1]: [d_date#40]
Arguments: HashedRelationBroadcastMode(List(input[0, date, true]),false), [plan_id=8]

(59) BroadcastHashJoin [codegen id : 3]
Left keys [1]: [d_date#39]
Right keys [1]: [d_date#40]
Join condition: None

(60) Project [codegen id : 3]
Output [1]: [d_date_sk#6]
Input [2]: [d_date_sk#6, d_date#39]

(61) BroadcastExchange
Input [1]: [d_date_sk#6]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=9]

Subquery:2 Hosting operator id = 16 Hosting Expression = cr_returned_date_sk#12 IN dynamicpruning#38

Subquery:3 Hosting operator id = 31 Hosting Expression = wr_returned_date_sk#23 IN dynamicpruning#38


