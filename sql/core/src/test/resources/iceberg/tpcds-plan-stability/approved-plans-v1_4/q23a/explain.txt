== Physical Plan ==
* HashAggregate (59)
+- Exchange (58)
   +- * HashAggregate (57)
      +- Union (56)
         :- * Project (43)
         :  +- * BroadcastHashJoin Inner BuildRight (42)
         :     :- * Project (40)
         :     :  +- * BroadcastHashJoin LeftSemi BuildRight (39)
         :     :     :- * Project (23)
         :     :     :  +- * BroadcastHashJoin LeftSemi BuildRight (22)
         :     :     :     :- * Project (3)
         :     :     :     :  +- * Filter (2)
         :     :     :     :     +- BatchScan spark_catalog.default.catalog_sales (1)
         :     :     :     +- BroadcastExchange (21)
         :     :     :        +- * Project (20)
         :     :     :           +- * Filter (19)
         :     :     :              +- * HashAggregate (18)
         :     :     :                 +- Exchange (17)
         :     :     :                    +- * HashAggregate (16)
         :     :     :                       +- * Project (15)
         :     :     :                          +- * BroadcastHashJoin Inner BuildRight (14)
         :     :     :                             :- * Project (9)
         :     :     :                             :  +- * BroadcastHashJoin Inner BuildRight (8)
         :     :     :                             :     :- * Project (6)
         :     :     :                             :     :  +- * Filter (5)
         :     :     :                             :     :     +- BatchScan spark_catalog.default.store_sales (4)
         :     :     :                             :     +- ReusedExchange (7)
         :     :     :                             +- BroadcastExchange (13)
         :     :     :                                +- * Project (12)
         :     :     :                                   +- * Filter (11)
         :     :     :                                      +- BatchScan spark_catalog.default.item (10)
         :     :     +- BroadcastExchange (38)
         :     :        +- * Project (37)
         :     :           +- * Filter (36)
         :     :              +- * HashAggregate (35)
         :     :                 +- Exchange (34)
         :     :                    +- * HashAggregate (33)
         :     :                       +- * Project (32)
         :     :                          +- * BroadcastHashJoin Inner BuildRight (31)
         :     :                             :- * Project (26)
         :     :                             :  +- * Filter (25)
         :     :                             :     +- BatchScan spark_catalog.default.store_sales (24)
         :     :                             +- BroadcastExchange (30)
         :     :                                +- * Project (29)
         :     :                                   +- * Filter (28)
         :     :                                      +- BatchScan spark_catalog.default.customer (27)
         :     +- ReusedExchange (41)
         +- * Project (55)
            +- * BroadcastHashJoin Inner BuildRight (54)
               :- * Project (52)
               :  +- * BroadcastHashJoin LeftSemi BuildRight (51)
               :     :- * Project (49)
               :     :  +- * BroadcastHashJoin LeftSemi BuildRight (48)
               :     :     :- * Project (46)
               :     :     :  +- * Filter (45)
               :     :     :     +- BatchScan spark_catalog.default.web_sales (44)
               :     :     +- ReusedExchange (47)
               :     +- ReusedExchange (50)
               +- ReusedExchange (53)


(1) BatchScan spark_catalog.default.catalog_sales
Output [5]: [cs_sold_date_sk#1, cs_bill_customer_sk#2, cs_item_sk#3, cs_quantity#4, cs_list_price#5]
spark_catalog.default.catalog_sales [scan class = SparkBatchQueryScan] [filters=cs_sold_date_sk IS NOT NULL], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(2) Filter [codegen id : 9]
Input [5]: [cs_sold_date_sk#1, cs_bill_customer_sk#2, cs_item_sk#3, cs_quantity#4, cs_list_price#5]
Condition : isnotnull(cs_sold_date_sk#1)

(3) Project [codegen id : 9]
Output [5]: [cs_sold_date_sk#1, cs_bill_customer_sk#2, cs_item_sk#3, cs_quantity#4, cs_list_price#5]
Input [5]: [cs_sold_date_sk#1, cs_bill_customer_sk#2, cs_item_sk#3, cs_quantity#4, cs_list_price#5]

(4) BatchScan spark_catalog.default.store_sales
Output [2]: [ss_sold_date_sk#6, ss_item_sk#7]
spark_catalog.default.store_sales [scan class = SparkBatchQueryScan] [filters=ss_sold_date_sk IS NOT NULL, ss_item_sk IS NOT NULL], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(5) Filter [codegen id : 3]
Input [2]: [ss_sold_date_sk#6, ss_item_sk#7]
Condition : (isnotnull(ss_sold_date_sk#6) AND isnotnull(ss_item_sk#7))

(6) Project [codegen id : 3]
Output [2]: [ss_sold_date_sk#6, ss_item_sk#7]
Input [2]: [ss_sold_date_sk#6, ss_item_sk#7]

(7) ReusedExchange [Reuses operator id: 67]
Output [2]: [d_date_sk#8, d_date#9]

(8) BroadcastHashJoin [codegen id : 3]
Left keys [1]: [ss_sold_date_sk#6]
Right keys [1]: [d_date_sk#8]
Join condition: None

(9) Project [codegen id : 3]
Output [2]: [ss_item_sk#7, d_date#9]
Input [4]: [ss_sold_date_sk#6, ss_item_sk#7, d_date_sk#8, d_date#9]

(10) BatchScan spark_catalog.default.item
Output [2]: [i_item_sk#10, i_item_desc#11]
spark_catalog.default.item [scan class = SparkBatchQueryScan] [filters=i_item_sk IS NOT NULL], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(11) Filter [codegen id : 2]
Input [2]: [i_item_sk#10, i_item_desc#11]
Condition : isnotnull(i_item_sk#10)

(12) Project [codegen id : 2]
Output [2]: [i_item_sk#10, i_item_desc#11]
Input [2]: [i_item_sk#10, i_item_desc#11]

(13) BroadcastExchange
Input [2]: [i_item_sk#10, i_item_desc#11]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=1]

(14) BroadcastHashJoin [codegen id : 3]
Left keys [1]: [ss_item_sk#7]
Right keys [1]: [i_item_sk#10]
Join condition: None

(15) Project [codegen id : 3]
Output [3]: [d_date#9, i_item_sk#10, substr(i_item_desc#11, 1, 30) AS _groupingexpression#12]
Input [4]: [ss_item_sk#7, d_date#9, i_item_sk#10, i_item_desc#11]

(16) HashAggregate [codegen id : 3]
Input [3]: [d_date#9, i_item_sk#10, _groupingexpression#12]
Keys [3]: [_groupingexpression#12, i_item_sk#10, d_date#9]
Functions [1]: [partial_count(1)]
Aggregate Attributes [1]: [count#13]
Results [4]: [_groupingexpression#12, i_item_sk#10, d_date#9, count#14]

(17) Exchange
Input [4]: [_groupingexpression#12, i_item_sk#10, d_date#9, count#14]
Arguments: hashpartitioning(_groupingexpression#12, i_item_sk#10, d_date#9, 5), ENSURE_REQUIREMENTS, [plan_id=2]

(18) HashAggregate [codegen id : 4]
Input [4]: [_groupingexpression#12, i_item_sk#10, d_date#9, count#14]
Keys [3]: [_groupingexpression#12, i_item_sk#10, d_date#9]
Functions [1]: [count(1)]
Aggregate Attributes [1]: [count(1)#15]
Results [2]: [i_item_sk#10 AS item_sk#16, count(1)#15 AS cnt#17]

(19) Filter [codegen id : 4]
Input [2]: [item_sk#16, cnt#17]
Condition : (cnt#17 > 4)

(20) Project [codegen id : 4]
Output [1]: [item_sk#16]
Input [2]: [item_sk#16, cnt#17]

(21) BroadcastExchange
Input [1]: [item_sk#16]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=3]

(22) BroadcastHashJoin [codegen id : 9]
Left keys [1]: [cs_item_sk#3]
Right keys [1]: [item_sk#16]
Join condition: None

(23) Project [codegen id : 9]
Output [4]: [cs_sold_date_sk#1, cs_bill_customer_sk#2, cs_quantity#4, cs_list_price#5]
Input [5]: [cs_sold_date_sk#1, cs_bill_customer_sk#2, cs_item_sk#3, cs_quantity#4, cs_list_price#5]

(24) BatchScan spark_catalog.default.store_sales
Output [3]: [ss_customer_sk#18, ss_quantity#19, ss_sales_price#20]
spark_catalog.default.store_sales [scan class = SparkBatchQueryScan] [filters=ss_customer_sk IS NOT NULL], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(25) Filter [codegen id : 6]
Input [3]: [ss_customer_sk#18, ss_quantity#19, ss_sales_price#20]
Condition : isnotnull(ss_customer_sk#18)

(26) Project [codegen id : 6]
Output [3]: [ss_customer_sk#18, ss_quantity#19, ss_sales_price#20]
Input [3]: [ss_customer_sk#18, ss_quantity#19, ss_sales_price#20]

(27) BatchScan spark_catalog.default.customer
Output [1]: [c_customer_sk#21]
spark_catalog.default.customer [scan class = SparkBatchQueryScan] [filters=c_customer_sk IS NOT NULL], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(28) Filter [codegen id : 5]
Input [1]: [c_customer_sk#21]
Condition : isnotnull(c_customer_sk#21)

(29) Project [codegen id : 5]
Output [1]: [c_customer_sk#21]
Input [1]: [c_customer_sk#21]

(30) BroadcastExchange
Input [1]: [c_customer_sk#21]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=4]

(31) BroadcastHashJoin [codegen id : 6]
Left keys [1]: [ss_customer_sk#18]
Right keys [1]: [c_customer_sk#21]
Join condition: None

(32) Project [codegen id : 6]
Output [3]: [ss_quantity#19, ss_sales_price#20, c_customer_sk#21]
Input [4]: [ss_customer_sk#18, ss_quantity#19, ss_sales_price#20, c_customer_sk#21]

(33) HashAggregate [codegen id : 6]
Input [3]: [ss_quantity#19, ss_sales_price#20, c_customer_sk#21]
Keys [1]: [c_customer_sk#21]
Functions [1]: [partial_sum(CheckOverflow((promote_precision(cast(ss_quantity#19 as decimal(12,2))) * promote_precision(cast(ss_sales_price#20 as decimal(12,2)))), DecimalType(18,2)))]
Aggregate Attributes [2]: [sum#22, isEmpty#23]
Results [3]: [c_customer_sk#21, sum#24, isEmpty#25]

(34) Exchange
Input [3]: [c_customer_sk#21, sum#24, isEmpty#25]
Arguments: hashpartitioning(c_customer_sk#21, 5), ENSURE_REQUIREMENTS, [plan_id=5]

(35) HashAggregate [codegen id : 7]
Input [3]: [c_customer_sk#21, sum#24, isEmpty#25]
Keys [1]: [c_customer_sk#21]
Functions [1]: [sum(CheckOverflow((promote_precision(cast(ss_quantity#19 as decimal(12,2))) * promote_precision(cast(ss_sales_price#20 as decimal(12,2)))), DecimalType(18,2)))]
Aggregate Attributes [1]: [sum(CheckOverflow((promote_precision(cast(ss_quantity#19 as decimal(12,2))) * promote_precision(cast(ss_sales_price#20 as decimal(12,2)))), DecimalType(18,2)))#26]
Results [2]: [c_customer_sk#21, sum(CheckOverflow((promote_precision(cast(ss_quantity#19 as decimal(12,2))) * promote_precision(cast(ss_sales_price#20 as decimal(12,2)))), DecimalType(18,2)))#26 AS ssales#27]

(36) Filter [codegen id : 7]
Input [2]: [c_customer_sk#21, ssales#27]
Condition : (isnotnull(ssales#27) AND (cast(ssales#27 as decimal(38,8)) > CheckOverflow((0.500000 * promote_precision(cast(Subquery scalar-subquery#28, [id=#29] as decimal(32,6)))), DecimalType(38,8))))

(37) Project [codegen id : 7]
Output [1]: [c_customer_sk#21]
Input [2]: [c_customer_sk#21, ssales#27]

(38) BroadcastExchange
Input [1]: [c_customer_sk#21]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=6]

(39) BroadcastHashJoin [codegen id : 9]
Left keys [1]: [cs_bill_customer_sk#2]
Right keys [1]: [c_customer_sk#21]
Join condition: None

(40) Project [codegen id : 9]
Output [3]: [cs_sold_date_sk#1, cs_quantity#4, cs_list_price#5]
Input [4]: [cs_sold_date_sk#1, cs_bill_customer_sk#2, cs_quantity#4, cs_list_price#5]

(41) ReusedExchange [Reuses operator id: 63]
Output [1]: [d_date_sk#30]

(42) BroadcastHashJoin [codegen id : 9]
Left keys [1]: [cs_sold_date_sk#1]
Right keys [1]: [d_date_sk#30]
Join condition: None

(43) Project [codegen id : 9]
Output [1]: [CheckOverflow((promote_precision(cast(cs_quantity#4 as decimal(12,2))) * promote_precision(cast(cs_list_price#5 as decimal(12,2)))), DecimalType(18,2)) AS sales#31]
Input [4]: [cs_sold_date_sk#1, cs_quantity#4, cs_list_price#5, d_date_sk#30]

(44) BatchScan spark_catalog.default.web_sales
Output [5]: [ws_sold_date_sk#32, ws_item_sk#33, ws_bill_customer_sk#34, ws_quantity#35, ws_list_price#36]
spark_catalog.default.web_sales [scan class = SparkBatchQueryScan] [filters=ws_sold_date_sk IS NOT NULL], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(45) Filter [codegen id : 18]
Input [5]: [ws_sold_date_sk#32, ws_item_sk#33, ws_bill_customer_sk#34, ws_quantity#35, ws_list_price#36]
Condition : isnotnull(ws_sold_date_sk#32)

(46) Project [codegen id : 18]
Output [5]: [ws_sold_date_sk#32, ws_item_sk#33, ws_bill_customer_sk#34, ws_quantity#35, ws_list_price#36]
Input [5]: [ws_sold_date_sk#32, ws_item_sk#33, ws_bill_customer_sk#34, ws_quantity#35, ws_list_price#36]

(47) ReusedExchange [Reuses operator id: 21]
Output [1]: [item_sk#16]

(48) BroadcastHashJoin [codegen id : 18]
Left keys [1]: [ws_item_sk#33]
Right keys [1]: [item_sk#16]
Join condition: None

(49) Project [codegen id : 18]
Output [4]: [ws_sold_date_sk#32, ws_bill_customer_sk#34, ws_quantity#35, ws_list_price#36]
Input [5]: [ws_sold_date_sk#32, ws_item_sk#33, ws_bill_customer_sk#34, ws_quantity#35, ws_list_price#36]

(50) ReusedExchange [Reuses operator id: 38]
Output [1]: [c_customer_sk#21]

(51) BroadcastHashJoin [codegen id : 18]
Left keys [1]: [ws_bill_customer_sk#34]
Right keys [1]: [c_customer_sk#21]
Join condition: None

(52) Project [codegen id : 18]
Output [3]: [ws_sold_date_sk#32, ws_quantity#35, ws_list_price#36]
Input [4]: [ws_sold_date_sk#32, ws_bill_customer_sk#34, ws_quantity#35, ws_list_price#36]

(53) ReusedExchange [Reuses operator id: 63]
Output [1]: [d_date_sk#37]

(54) BroadcastHashJoin [codegen id : 18]
Left keys [1]: [ws_sold_date_sk#32]
Right keys [1]: [d_date_sk#37]
Join condition: None

(55) Project [codegen id : 18]
Output [1]: [CheckOverflow((promote_precision(cast(ws_quantity#35 as decimal(12,2))) * promote_precision(cast(ws_list_price#36 as decimal(12,2)))), DecimalType(18,2)) AS sales#38]
Input [4]: [ws_sold_date_sk#32, ws_quantity#35, ws_list_price#36, d_date_sk#37]

(56) Union

(57) HashAggregate [codegen id : 19]
Input [1]: [sales#31]
Keys: []
Functions [1]: [partial_sum(sales#31)]
Aggregate Attributes [2]: [sum#39, isEmpty#40]
Results [2]: [sum#41, isEmpty#42]

(58) Exchange
Input [2]: [sum#41, isEmpty#42]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=7]

(59) HashAggregate [codegen id : 20]
Input [2]: [sum#41, isEmpty#42]
Keys: []
Functions [1]: [sum(sales#31)]
Aggregate Attributes [1]: [sum(sales#31)#43]
Results [1]: [sum(sales#31)#43 AS sum(sales)#44]

===== Subqueries =====

Subquery:1 Hosting operator id = 1 Hosting Expression = cs_sold_date_sk#1 IN dynamicpruning#45
BroadcastExchange (63)
+- * Project (62)
   +- * Filter (61)
      +- BatchScan spark_catalog.default.date_dim (60)


(60) BatchScan spark_catalog.default.date_dim
Output [3]: [d_date_sk#30, d_year#46, d_moy#47]
spark_catalog.default.date_dim [scan class = SparkBatchQueryScan] [filters=d_year IS NOT NULL, d_moy IS NOT NULL, d_year = 2000, d_moy = 2, d_date_sk IS NOT NULL], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(61) Filter [codegen id : 1]
Input [3]: [d_date_sk#30, d_year#46, d_moy#47]
Condition : ((((isnotnull(d_year#46) AND isnotnull(d_moy#47)) AND (d_year#46 = 2000)) AND (d_moy#47 = 2)) AND isnotnull(d_date_sk#30))

(62) Project [codegen id : 1]
Output [1]: [d_date_sk#30]
Input [3]: [d_date_sk#30, d_year#46, d_moy#47]

(63) BroadcastExchange
Input [1]: [d_date_sk#30]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=8]

Subquery:2 Hosting operator id = 4 Hosting Expression = ss_sold_date_sk#6 IN dynamicpruning#48
BroadcastExchange (67)
+- * Project (66)
   +- * Filter (65)
      +- BatchScan spark_catalog.default.date_dim (64)


(64) BatchScan spark_catalog.default.date_dim
Output [3]: [d_date_sk#8, d_date#9, d_year#49]
spark_catalog.default.date_dim [scan class = SparkBatchQueryScan] [filters=d_year IN (2000, 2001, 2002, 2003), d_date_sk IS NOT NULL], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(65) Filter [codegen id : 1]
Input [3]: [d_date_sk#8, d_date#9, d_year#49]
Condition : (d_year#49 IN (2000,2001,2002,2003) AND isnotnull(d_date_sk#8))

(66) Project [codegen id : 1]
Output [2]: [d_date_sk#8, d_date#9]
Input [3]: [d_date_sk#8, d_date#9, d_year#49]

(67) BroadcastExchange
Input [2]: [d_date_sk#8, d_date#9]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=9]

Subquery:3 Hosting operator id = 36 Hosting Expression = Subquery scalar-subquery#28, [id=#29]
* HashAggregate (82)
+- Exchange (81)
   +- * HashAggregate (80)
      +- * HashAggregate (79)
         +- Exchange (78)
            +- * HashAggregate (77)
               +- * Project (76)
                  +- * BroadcastHashJoin Inner BuildRight (75)
                     :- * Project (73)
                     :  +- * BroadcastHashJoin Inner BuildRight (72)
                     :     :- * Project (70)
                     :     :  +- * Filter (69)
                     :     :     +- BatchScan spark_catalog.default.store_sales (68)
                     :     +- ReusedExchange (71)
                     +- ReusedExchange (74)


(68) BatchScan spark_catalog.default.store_sales
Output [4]: [ss_sold_date_sk#50, ss_customer_sk#51, ss_quantity#52, ss_sales_price#53]
spark_catalog.default.store_sales [scan class = SparkBatchQueryScan] [filters=ss_customer_sk IS NOT NULL, ss_sold_date_sk IS NOT NULL], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(69) Filter [codegen id : 3]
Input [4]: [ss_sold_date_sk#50, ss_customer_sk#51, ss_quantity#52, ss_sales_price#53]
Condition : (isnotnull(ss_customer_sk#51) AND isnotnull(ss_sold_date_sk#50))

(70) Project [codegen id : 3]
Output [4]: [ss_sold_date_sk#50, ss_customer_sk#51, ss_quantity#52, ss_sales_price#53]
Input [4]: [ss_sold_date_sk#50, ss_customer_sk#51, ss_quantity#52, ss_sales_price#53]

(71) ReusedExchange [Reuses operator id: 30]
Output [1]: [c_customer_sk#54]

(72) BroadcastHashJoin [codegen id : 3]
Left keys [1]: [ss_customer_sk#51]
Right keys [1]: [c_customer_sk#54]
Join condition: None

(73) Project [codegen id : 3]
Output [4]: [ss_sold_date_sk#50, ss_quantity#52, ss_sales_price#53, c_customer_sk#54]
Input [5]: [ss_sold_date_sk#50, ss_customer_sk#51, ss_quantity#52, ss_sales_price#53, c_customer_sk#54]

(74) ReusedExchange [Reuses operator id: 86]
Output [1]: [d_date_sk#55]

(75) BroadcastHashJoin [codegen id : 3]
Left keys [1]: [ss_sold_date_sk#50]
Right keys [1]: [d_date_sk#55]
Join condition: None

(76) Project [codegen id : 3]
Output [3]: [ss_quantity#52, ss_sales_price#53, c_customer_sk#54]
Input [5]: [ss_sold_date_sk#50, ss_quantity#52, ss_sales_price#53, c_customer_sk#54, d_date_sk#55]

(77) HashAggregate [codegen id : 3]
Input [3]: [ss_quantity#52, ss_sales_price#53, c_customer_sk#54]
Keys [1]: [c_customer_sk#54]
Functions [1]: [partial_sum(CheckOverflow((promote_precision(cast(ss_quantity#52 as decimal(12,2))) * promote_precision(cast(ss_sales_price#53 as decimal(12,2)))), DecimalType(18,2)))]
Aggregate Attributes [2]: [sum#56, isEmpty#57]
Results [3]: [c_customer_sk#54, sum#58, isEmpty#59]

(78) Exchange
Input [3]: [c_customer_sk#54, sum#58, isEmpty#59]
Arguments: hashpartitioning(c_customer_sk#54, 5), ENSURE_REQUIREMENTS, [plan_id=10]

(79) HashAggregate [codegen id : 4]
Input [3]: [c_customer_sk#54, sum#58, isEmpty#59]
Keys [1]: [c_customer_sk#54]
Functions [1]: [sum(CheckOverflow((promote_precision(cast(ss_quantity#52 as decimal(12,2))) * promote_precision(cast(ss_sales_price#53 as decimal(12,2)))), DecimalType(18,2)))]
Aggregate Attributes [1]: [sum(CheckOverflow((promote_precision(cast(ss_quantity#52 as decimal(12,2))) * promote_precision(cast(ss_sales_price#53 as decimal(12,2)))), DecimalType(18,2)))#60]
Results [1]: [sum(CheckOverflow((promote_precision(cast(ss_quantity#52 as decimal(12,2))) * promote_precision(cast(ss_sales_price#53 as decimal(12,2)))), DecimalType(18,2)))#60 AS csales#61]

(80) HashAggregate [codegen id : 4]
Input [1]: [csales#61]
Keys: []
Functions [1]: [partial_max(csales#61)]
Aggregate Attributes [1]: [max#62]
Results [1]: [max#63]

(81) Exchange
Input [1]: [max#63]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=11]

(82) HashAggregate [codegen id : 5]
Input [1]: [max#63]
Keys: []
Functions [1]: [max(csales#61)]
Aggregate Attributes [1]: [max(csales#61)#64]
Results [1]: [max(csales#61)#64 AS tpcds_cmax#65]

Subquery:4 Hosting operator id = 68 Hosting Expression = ss_sold_date_sk#50 IN dynamicpruning#66
BroadcastExchange (86)
+- * Project (85)
   +- * Filter (84)
      +- BatchScan spark_catalog.default.date_dim (83)


(83) BatchScan spark_catalog.default.date_dim
Output [2]: [d_date_sk#55, d_year#67]
spark_catalog.default.date_dim [scan class = SparkBatchQueryScan] [filters=d_year IN (2000, 2001, 2002, 2003), d_date_sk IS NOT NULL], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(84) Filter [codegen id : 1]
Input [2]: [d_date_sk#55, d_year#67]
Condition : (d_year#67 IN (2000,2001,2002,2003) AND isnotnull(d_date_sk#55))

(85) Project [codegen id : 1]
Output [1]: [d_date_sk#55]
Input [2]: [d_date_sk#55, d_year#67]

(86) BroadcastExchange
Input [1]: [d_date_sk#55]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=12]

Subquery:5 Hosting operator id = 44 Hosting Expression = ws_sold_date_sk#32 IN dynamicpruning#45


