== Physical Plan ==
TakeOrderedAndProject (53)
+- * HashAggregate (52)
   +- Exchange (51)
      +- * HashAggregate (50)
         +- * HashAggregate (49)
            +- Exchange (48)
               +- * HashAggregate (47)
                  +- * Project (46)
                     +- * BroadcastHashJoin Inner BuildRight (45)
                        :- * Project (43)
                        :  +- * BroadcastHashJoin Inner BuildRight (42)
                        :     :- * Project (37)
                        :     :  +- * BroadcastHashJoin Inner BuildRight (36)
                        :     :     :- * Project (31)
                        :     :     :  +- * BroadcastHashJoin Inner BuildRight (30)
                        :     :     :     :- * HashAggregate (25)
                        :     :     :     :  +- Exchange (24)
                        :     :     :     :     +- * HashAggregate (23)
                        :     :     :     :        +- * Project (22)
                        :     :     :     :           +- * BroadcastHashJoin Inner BuildRight (21)
                        :     :     :     :              :- * Project (16)
                        :     :     :     :              :  +- * BroadcastHashJoin Inner BuildRight (15)
                        :     :     :     :              :     :- * Project (13)
                        :     :     :     :              :     :  +- * BroadcastHashJoin Inner BuildRight (12)
                        :     :     :     :              :     :     :- Union (7)
                        :     :     :     :              :     :     :  :- * Project (3)
                        :     :     :     :              :     :     :  :  +- * Filter (2)
                        :     :     :     :              :     :     :  :     +- BatchScan spark_catalog.default.catalog_sales (1)
                        :     :     :     :              :     :     :  +- * Project (6)
                        :     :     :     :              :     :     :     +- * Filter (5)
                        :     :     :     :              :     :     :        +- BatchScan spark_catalog.default.web_sales (4)
                        :     :     :     :              :     :     +- BroadcastExchange (11)
                        :     :     :     :              :     :        +- * Project (10)
                        :     :     :     :              :     :           +- * Filter (9)
                        :     :     :     :              :     :              +- BatchScan spark_catalog.default.item (8)
                        :     :     :     :              :     +- ReusedExchange (14)
                        :     :     :     :              +- BroadcastExchange (20)
                        :     :     :     :                 +- * Project (19)
                        :     :     :     :                    +- * Filter (18)
                        :     :     :     :                       +- BatchScan spark_catalog.default.customer (17)
                        :     :     :     +- BroadcastExchange (29)
                        :     :     :        +- * Project (28)
                        :     :     :           +- * Filter (27)
                        :     :     :              +- BatchScan spark_catalog.default.store_sales (26)
                        :     :     +- BroadcastExchange (35)
                        :     :        +- * Project (34)
                        :     :           +- * Filter (33)
                        :     :              +- BatchScan spark_catalog.default.customer_address (32)
                        :     +- BroadcastExchange (41)
                        :        +- * Project (40)
                        :           +- * Filter (39)
                        :              +- BatchScan spark_catalog.default.store (38)
                        +- ReusedExchange (44)


(1) BatchScan spark_catalog.default.catalog_sales
Output [3]: [cs_sold_date_sk#1, cs_bill_customer_sk#2, cs_item_sk#3]
spark_catalog.default.catalog_sales [scan class = SparkBatchQueryScan] [filters=cs_item_sk IS NOT NULL, cs_sold_date_sk IS NOT NULL, cs_bill_customer_sk IS NOT NULL], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(2) Filter [codegen id : 1]
Input [3]: [cs_sold_date_sk#1, cs_bill_customer_sk#2, cs_item_sk#3]
Condition : ((isnotnull(cs_item_sk#3) AND isnotnull(cs_sold_date_sk#1)) AND isnotnull(cs_bill_customer_sk#2))

(3) Project [codegen id : 1]
Output [3]: [cs_sold_date_sk#1 AS sold_date_sk#4, cs_bill_customer_sk#2 AS customer_sk#5, cs_item_sk#3 AS item_sk#6]
Input [3]: [cs_sold_date_sk#1, cs_bill_customer_sk#2, cs_item_sk#3]

(4) BatchScan spark_catalog.default.web_sales
Output [3]: [ws_sold_date_sk#7, ws_item_sk#8, ws_bill_customer_sk#9]
spark_catalog.default.web_sales [scan class = SparkBatchQueryScan] [filters=ws_item_sk IS NOT NULL, ws_sold_date_sk IS NOT NULL, ws_bill_customer_sk IS NOT NULL], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(5) Filter [codegen id : 2]
Input [3]: [ws_sold_date_sk#7, ws_item_sk#8, ws_bill_customer_sk#9]
Condition : ((isnotnull(ws_item_sk#8) AND isnotnull(ws_sold_date_sk#7)) AND isnotnull(ws_bill_customer_sk#9))

(6) Project [codegen id : 2]
Output [3]: [ws_sold_date_sk#7 AS sold_date_sk#10, ws_bill_customer_sk#9 AS customer_sk#11, ws_item_sk#8 AS item_sk#12]
Input [3]: [ws_sold_date_sk#7, ws_item_sk#8, ws_bill_customer_sk#9]

(7) Union

(8) BatchScan spark_catalog.default.item
Output [3]: [i_item_sk#13, i_class#14, i_category#15]
spark_catalog.default.item [scan class = SparkBatchQueryScan] [filters=i_category IS NOT NULL, i_class IS NOT NULL, i_category = 'Women', i_class = 'maternity', i_item_sk IS NOT NULL], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(9) Filter [codegen id : 3]
Input [3]: [i_item_sk#13, i_class#14, i_category#15]
Condition : ((((isnotnull(i_category#15) AND isnotnull(i_class#14)) AND (i_category#15 = Women)) AND (i_class#14 = maternity)) AND isnotnull(i_item_sk#13))

(10) Project [codegen id : 3]
Output [1]: [i_item_sk#13]
Input [3]: [i_item_sk#13, i_class#14, i_category#15]

(11) BroadcastExchange
Input [1]: [i_item_sk#13]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=1]

(12) BroadcastHashJoin [codegen id : 6]
Left keys [1]: [item_sk#6]
Right keys [1]: [i_item_sk#13]
Join condition: None

(13) Project [codegen id : 6]
Output [2]: [sold_date_sk#4, customer_sk#5]
Input [4]: [sold_date_sk#4, customer_sk#5, item_sk#6, i_item_sk#13]

(14) ReusedExchange [Reuses operator id: 57]
Output [1]: [d_date_sk#16]

(15) BroadcastHashJoin [codegen id : 6]
Left keys [1]: [sold_date_sk#4]
Right keys [1]: [d_date_sk#16]
Join condition: None

(16) Project [codegen id : 6]
Output [1]: [customer_sk#5]
Input [3]: [sold_date_sk#4, customer_sk#5, d_date_sk#16]

(17) BatchScan spark_catalog.default.customer
Output [2]: [c_customer_sk#17, c_current_addr_sk#18]
spark_catalog.default.customer [scan class = SparkBatchQueryScan] [filters=c_customer_sk IS NOT NULL, c_current_addr_sk IS NOT NULL], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(18) Filter [codegen id : 5]
Input [2]: [c_customer_sk#17, c_current_addr_sk#18]
Condition : (isnotnull(c_customer_sk#17) AND isnotnull(c_current_addr_sk#18))

(19) Project [codegen id : 5]
Output [2]: [c_customer_sk#17, c_current_addr_sk#18]
Input [2]: [c_customer_sk#17, c_current_addr_sk#18]

(20) BroadcastExchange
Input [2]: [c_customer_sk#17, c_current_addr_sk#18]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=2]

(21) BroadcastHashJoin [codegen id : 6]
Left keys [1]: [customer_sk#5]
Right keys [1]: [c_customer_sk#17]
Join condition: None

(22) Project [codegen id : 6]
Output [2]: [c_customer_sk#17, c_current_addr_sk#18]
Input [3]: [customer_sk#5, c_customer_sk#17, c_current_addr_sk#18]

(23) HashAggregate [codegen id : 6]
Input [2]: [c_customer_sk#17, c_current_addr_sk#18]
Keys [2]: [c_customer_sk#17, c_current_addr_sk#18]
Functions: []
Aggregate Attributes: []
Results [2]: [c_customer_sk#17, c_current_addr_sk#18]

(24) Exchange
Input [2]: [c_customer_sk#17, c_current_addr_sk#18]
Arguments: hashpartitioning(c_customer_sk#17, c_current_addr_sk#18, 5), ENSURE_REQUIREMENTS, [plan_id=3]

(25) HashAggregate [codegen id : 11]
Input [2]: [c_customer_sk#17, c_current_addr_sk#18]
Keys [2]: [c_customer_sk#17, c_current_addr_sk#18]
Functions: []
Aggregate Attributes: []
Results [2]: [c_customer_sk#17, c_current_addr_sk#18]

(26) BatchScan spark_catalog.default.store_sales
Output [3]: [ss_sold_date_sk#19, ss_customer_sk#20, ss_ext_sales_price#21]
spark_catalog.default.store_sales [scan class = SparkBatchQueryScan] [filters=ss_customer_sk IS NOT NULL, ss_sold_date_sk IS NOT NULL], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(27) Filter [codegen id : 7]
Input [3]: [ss_sold_date_sk#19, ss_customer_sk#20, ss_ext_sales_price#21]
Condition : (isnotnull(ss_customer_sk#20) AND isnotnull(ss_sold_date_sk#19))

(28) Project [codegen id : 7]
Output [3]: [ss_sold_date_sk#19, ss_customer_sk#20, ss_ext_sales_price#21]
Input [3]: [ss_sold_date_sk#19, ss_customer_sk#20, ss_ext_sales_price#21]

(29) BroadcastExchange
Input [3]: [ss_sold_date_sk#19, ss_customer_sk#20, ss_ext_sales_price#21]
Arguments: HashedRelationBroadcastMode(List(cast(input[1, int, true] as bigint)),false), [plan_id=4]

(30) BroadcastHashJoin [codegen id : 11]
Left keys [1]: [c_customer_sk#17]
Right keys [1]: [ss_customer_sk#20]
Join condition: None

(31) Project [codegen id : 11]
Output [4]: [c_customer_sk#17, c_current_addr_sk#18, ss_sold_date_sk#19, ss_ext_sales_price#21]
Input [5]: [c_customer_sk#17, c_current_addr_sk#18, ss_sold_date_sk#19, ss_customer_sk#20, ss_ext_sales_price#21]

(32) BatchScan spark_catalog.default.customer_address
Output [3]: [ca_address_sk#22, ca_county#23, ca_state#24]
spark_catalog.default.customer_address [scan class = SparkBatchQueryScan] [filters=ca_address_sk IS NOT NULL, ca_county IS NOT NULL, ca_state IS NOT NULL], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(33) Filter [codegen id : 8]
Input [3]: [ca_address_sk#22, ca_county#23, ca_state#24]
Condition : ((isnotnull(ca_address_sk#22) AND isnotnull(ca_county#23)) AND isnotnull(ca_state#24))

(34) Project [codegen id : 8]
Output [3]: [ca_address_sk#22, ca_county#23, ca_state#24]
Input [3]: [ca_address_sk#22, ca_county#23, ca_state#24]

(35) BroadcastExchange
Input [3]: [ca_address_sk#22, ca_county#23, ca_state#24]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=5]

(36) BroadcastHashJoin [codegen id : 11]
Left keys [1]: [c_current_addr_sk#18]
Right keys [1]: [ca_address_sk#22]
Join condition: None

(37) Project [codegen id : 11]
Output [5]: [c_customer_sk#17, ss_sold_date_sk#19, ss_ext_sales_price#21, ca_county#23, ca_state#24]
Input [7]: [c_customer_sk#17, c_current_addr_sk#18, ss_sold_date_sk#19, ss_ext_sales_price#21, ca_address_sk#22, ca_county#23, ca_state#24]

(38) BatchScan spark_catalog.default.store
Output [2]: [s_county#25, s_state#26]
spark_catalog.default.store [scan class = SparkBatchQueryScan] [filters=s_county IS NOT NULL, s_state IS NOT NULL], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(39) Filter [codegen id : 9]
Input [2]: [s_county#25, s_state#26]
Condition : (isnotnull(s_county#25) AND isnotnull(s_state#26))

(40) Project [codegen id : 9]
Output [2]: [s_county#25, s_state#26]
Input [2]: [s_county#25, s_state#26]

(41) BroadcastExchange
Input [2]: [s_county#25, s_state#26]
Arguments: HashedRelationBroadcastMode(List(input[0, string, true], input[1, string, true]),false), [plan_id=6]

(42) BroadcastHashJoin [codegen id : 11]
Left keys [2]: [ca_county#23, ca_state#24]
Right keys [2]: [s_county#25, s_state#26]
Join condition: None

(43) Project [codegen id : 11]
Output [3]: [c_customer_sk#17, ss_sold_date_sk#19, ss_ext_sales_price#21]
Input [7]: [c_customer_sk#17, ss_sold_date_sk#19, ss_ext_sales_price#21, ca_county#23, ca_state#24, s_county#25, s_state#26]

(44) ReusedExchange [Reuses operator id: 61]
Output [1]: [d_date_sk#27]

(45) BroadcastHashJoin [codegen id : 11]
Left keys [1]: [ss_sold_date_sk#19]
Right keys [1]: [d_date_sk#27]
Join condition: None

(46) Project [codegen id : 11]
Output [2]: [c_customer_sk#17, ss_ext_sales_price#21]
Input [4]: [c_customer_sk#17, ss_sold_date_sk#19, ss_ext_sales_price#21, d_date_sk#27]

(47) HashAggregate [codegen id : 11]
Input [2]: [c_customer_sk#17, ss_ext_sales_price#21]
Keys [1]: [c_customer_sk#17]
Functions [1]: [partial_sum(UnscaledValue(ss_ext_sales_price#21))]
Aggregate Attributes [1]: [sum#28]
Results [2]: [c_customer_sk#17, sum#29]

(48) Exchange
Input [2]: [c_customer_sk#17, sum#29]
Arguments: hashpartitioning(c_customer_sk#17, 5), ENSURE_REQUIREMENTS, [plan_id=7]

(49) HashAggregate [codegen id : 12]
Input [2]: [c_customer_sk#17, sum#29]
Keys [1]: [c_customer_sk#17]
Functions [1]: [sum(UnscaledValue(ss_ext_sales_price#21))]
Aggregate Attributes [1]: [sum(UnscaledValue(ss_ext_sales_price#21))#30]
Results [1]: [cast(CheckOverflow((promote_precision(MakeDecimal(sum(UnscaledValue(ss_ext_sales_price#21))#30,17,2)) / 50.00), DecimalType(21,6)) as int) AS segment#31]

(50) HashAggregate [codegen id : 12]
Input [1]: [segment#31]
Keys [1]: [segment#31]
Functions [1]: [partial_count(1)]
Aggregate Attributes [1]: [count#32]
Results [2]: [segment#31, count#33]

(51) Exchange
Input [2]: [segment#31, count#33]
Arguments: hashpartitioning(segment#31, 5), ENSURE_REQUIREMENTS, [plan_id=8]

(52) HashAggregate [codegen id : 13]
Input [2]: [segment#31, count#33]
Keys [1]: [segment#31]
Functions [1]: [count(1)]
Aggregate Attributes [1]: [count(1)#34]
Results [3]: [segment#31, count(1)#34 AS num_customers#35, (segment#31 * 50) AS segment_base#36]

(53) TakeOrderedAndProject
Input [3]: [segment#31, num_customers#35, segment_base#36]
Arguments: 100, [segment#31 ASC NULLS FIRST, num_customers#35 ASC NULLS FIRST], [segment#31, num_customers#35, segment_base#36]

===== Subqueries =====

Subquery:1 Hosting operator id = 1 Hosting Expression = cs_sold_date_sk#1 IN dynamicpruning#37
BroadcastExchange (57)
+- * Project (56)
   +- * Filter (55)
      +- BatchScan spark_catalog.default.date_dim (54)


(54) BatchScan spark_catalog.default.date_dim
Output [3]: [d_date_sk#16, d_year#38, d_moy#39]
spark_catalog.default.date_dim [scan class = SparkBatchQueryScan] [filters=d_moy IS NOT NULL, d_year IS NOT NULL, d_moy = 12, d_year = 1998, d_date_sk IS NOT NULL], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(55) Filter [codegen id : 1]
Input [3]: [d_date_sk#16, d_year#38, d_moy#39]
Condition : ((((isnotnull(d_moy#39) AND isnotnull(d_year#38)) AND (d_moy#39 = 12)) AND (d_year#38 = 1998)) AND isnotnull(d_date_sk#16))

(56) Project [codegen id : 1]
Output [1]: [d_date_sk#16]
Input [3]: [d_date_sk#16, d_year#38, d_moy#39]

(57) BroadcastExchange
Input [1]: [d_date_sk#16]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=9]

Subquery:2 Hosting operator id = 4 Hosting Expression = ws_sold_date_sk#7 IN dynamicpruning#37

Subquery:3 Hosting operator id = 26 Hosting Expression = ss_sold_date_sk#19 IN dynamicpruning#40
BroadcastExchange (61)
+- * Project (60)
   +- * Filter (59)
      +- BatchScan spark_catalog.default.date_dim (58)


(58) BatchScan spark_catalog.default.date_dim
Output [2]: [d_date_sk#27, d_month_seq#41]
spark_catalog.default.date_dim [scan class = SparkBatchQueryScan] [filters=d_month_seq IS NOT NULL, d_date_sk IS NOT NULL], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(59) Filter [codegen id : 1]
Input [2]: [d_date_sk#27, d_month_seq#41]
Condition : (((isnotnull(d_month_seq#41) AND isnotnull(d_date_sk#27)) AND (d_month_seq#41 >= Subquery scalar-subquery#42, [id=#43])) AND (d_month_seq#41 <= Subquery scalar-subquery#44, [id=#45]))

(60) Project [codegen id : 1]
Output [1]: [d_date_sk#27]
Input [2]: [d_date_sk#27, d_month_seq#41]

(61) BroadcastExchange
Input [1]: [d_date_sk#27]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=10]

Subquery:4 Hosting operator id = 59 Hosting Expression = Subquery scalar-subquery#42, [id=#43]
* HashAggregate (67)
+- Exchange (66)
   +- * HashAggregate (65)
      +- * Project (64)
         +- * Filter (63)
            +- BatchScan spark_catalog.default.date_dim (62)


(62) BatchScan spark_catalog.default.date_dim
Output [3]: [d_month_seq#46, d_year#47, d_moy#48]
spark_catalog.default.date_dim [scan class = SparkBatchQueryScan] [filters=d_year IS NOT NULL, d_moy IS NOT NULL, d_year = 1998, d_moy = 12], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(63) Filter [codegen id : 1]
Input [3]: [d_month_seq#46, d_year#47, d_moy#48]
Condition : (((isnotnull(d_year#47) AND isnotnull(d_moy#48)) AND (d_year#47 = 1998)) AND (d_moy#48 = 12))

(64) Project [codegen id : 1]
Output [1]: [(d_month_seq#46 + 1) AS (d_month_seq + 1)#49]
Input [3]: [d_month_seq#46, d_year#47, d_moy#48]

(65) HashAggregate [codegen id : 1]
Input [1]: [(d_month_seq + 1)#49]
Keys [1]: [(d_month_seq + 1)#49]
Functions: []
Aggregate Attributes: []
Results [1]: [(d_month_seq + 1)#49]

(66) Exchange
Input [1]: [(d_month_seq + 1)#49]
Arguments: hashpartitioning((d_month_seq + 1)#49, 5), ENSURE_REQUIREMENTS, [plan_id=11]

(67) HashAggregate [codegen id : 2]
Input [1]: [(d_month_seq + 1)#49]
Keys [1]: [(d_month_seq + 1)#49]
Functions: []
Aggregate Attributes: []
Results [1]: [(d_month_seq + 1)#49]

Subquery:5 Hosting operator id = 59 Hosting Expression = Subquery scalar-subquery#44, [id=#45]
* HashAggregate (73)
+- Exchange (72)
   +- * HashAggregate (71)
      +- * Project (70)
         +- * Filter (69)
            +- BatchScan spark_catalog.default.date_dim (68)


(68) BatchScan spark_catalog.default.date_dim
Output [3]: [d_month_seq#50, d_year#51, d_moy#52]
spark_catalog.default.date_dim [scan class = SparkBatchQueryScan] [filters=d_year IS NOT NULL, d_moy IS NOT NULL, d_year = 1998, d_moy = 12], [runtimeFilters=[]], caseSensitive=false,[ Broadcast Var UNUSED =]

(69) Filter [codegen id : 1]
Input [3]: [d_month_seq#50, d_year#51, d_moy#52]
Condition : (((isnotnull(d_year#51) AND isnotnull(d_moy#52)) AND (d_year#51 = 1998)) AND (d_moy#52 = 12))

(70) Project [codegen id : 1]
Output [1]: [(d_month_seq#50 + 3) AS (d_month_seq + 3)#53]
Input [3]: [d_month_seq#50, d_year#51, d_moy#52]

(71) HashAggregate [codegen id : 1]
Input [1]: [(d_month_seq + 3)#53]
Keys [1]: [(d_month_seq + 3)#53]
Functions: []
Aggregate Attributes: []
Results [1]: [(d_month_seq + 3)#53]

(72) Exchange
Input [1]: [(d_month_seq + 3)#53]
Arguments: hashpartitioning((d_month_seq + 3)#53, 5), ENSURE_REQUIREMENTS, [plan_id=12]

(73) HashAggregate [codegen id : 2]
Input [1]: [(d_month_seq + 3)#53]
Keys [1]: [(d_month_seq + 3)#53]
Functions: []
Aggregate Attributes: []
Results [1]: [(d_month_seq + 3)#53]


